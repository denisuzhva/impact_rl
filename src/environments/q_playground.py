import cv2
import numpy as np
import src.FEM.solve_2 as solvers
import itertools



class action_space:
    """
    Simple action space class. 
    """
    def __init__(self, n_actions) -> None:
        self.init_n_actions = n_actions
        self.n_legal_actions = n_actions
        self.all_action_list = np.arange(self.n_legal_actions)
        self.legal_action_mask = np.ones_like(self.all_action_list)

    def adjust_legal(self, action):
        self.legal_action_mask[action] = 0
        self.n_legal_actions -= 1
        assert self.n_legal_actions >= 0

    def sample(self):
        legal_actions = self.all_action_list[self.legal_action_mask == 1]
        action = np.random.choice(legal_actions)
        return action
    
    def reset(self):
        self.legal_action_mask = np.ones_like(self.all_action_list)
        self.n_legal_actions = self.init_n_actions


class ImageEnvV1:
    """
    Environment generated by an picture.
    """
    id_iter = itertools.count()

    def __init__(self, 
                 img_path='C:/dev/_spbu/impact_rl/env_data/init_patterns/chessboard_10x10.png',
                 reward_weight=1.,
                 flatten=True):
        """
        Args:
            img_path:       Path to an image which encodes a target pattern
            reward_weight:  Reward multiplier
        """
        self.id = next(ImageEnvV1.id_iter)
        env_space = cv2.imread(img_path)
        env_space_g = cv2.cvtColor(env_space, cv2.COLOR_BGR2GRAY)

        self.reward_weight = reward_weight
        self.h, self.w = env_space_g.shape
        if flatten:
            env_space_ready = env_space_g.flatten()
        else:
            env_space_ready = env_space_g

        self.state_dim = env_space_ready.shape
        self.state_size = env_space_ready.size

        #self.reward_map_init = reward_weight * (2 * (env_space_ready / 255) - 1)
        self.reward_map = reward_weight * (2 * (env_space_ready / 255) - 1)
        #self.reward_map = np.copy(self.reward_map_init)

        self.target_state = (env_space_ready / 255).astype(np.float32)
        self.state = np.zeros_like(self.reward_map, dtype=np.float32)
        self.state_prior_reset = np.zeros_like(self.reward_map, dtype=np.float32)
        self.action_space = action_space(self.state_size)

    def step(self, action):
        new_state = self.state
        new_state[action] = 1.
        reward = self.reward_map[action]
        #self.reward_map[action] = -1 * self.reward_weight
        self.state = new_state
        if np.array_equal(self.state, self.target_state) or (self.state.sum() == self.state_size):
            is_done = True
        else:
            is_done = False
        return new_state, reward, is_done, None

    def reset(self):
        self.state_prior_reset = self.state
        self.state = np.zeros_like(self.state)
        self.action_space.reset()
        #self.reward_map = np.copy(self.reward_map_init)
        return self.state
    
    def get_state(self):
        return self.state.reshape((self.h, self.w))
        

class SimEnvV1:
    """
    Impact speed minimization.
    """
    id_iter = itertools.count()

    def __init__(self, 
                 solver_func=solvers.solve_batch,
                 fem_env_data_dir='C:/dev/_spbu/impact_rl/env_data/FEM/',
                 target_vel_read=False,
                 ncpu=12,
                 h=3,
                 w=6,
                 reward_weight=1.,
                 inverse_problem=False,
                 flatten=True):
        """
        Args:
            solver_func:        FEM solver
            fem_env_data_dir:   Directory to velocity target values and optimal topology dump
            target_vel_read:    True if read target velocity from fem_env_data_dir;
                                False if compute target velocity by the solver
            ncpu:               Number of CPU threads for the solver
            h, w:               Environment space height and width
            reward_weight:      Reward multiplier
            inverse_problem:    True if solve a velocity maximization problem; 
                                False if solve a minimization one
            flatten:            True if flatten the state space
        """
        self.id = next(SimEnvV1.id_iter)
        self.reward_weight = reward_weight
        self.h = h 
        self.w = w
        self.solver = solver_func 
        if flatten:
            env_space_ready = np.zeros((h, w)).flatten()
        else:
            env_space_ready = np.zeros((h, w))

        self.state_dim = env_space_ready.shape
        self.state_size = env_space_ready.size

        self.state = np.zeros_like(env_space_ready, dtype=np.float32)
        self.state_prior_reset = np.zeros_like(env_space_ready, dtype=np.float32)
        self.action_space = action_space(self.state_size)

        self.ncpu = ncpu
        self.fem_env_data_dir = fem_env_data_dir
        if target_vel_read:
            self.target_vel = np.loadtxt(self.fem_env_data_dir + 'vel.txt')
            #print(self.target_vel)
        else:
            self.target_vel =  self.solver([self.state], self.id, ncpu_mp=1, ncpu_solver=self.ncpu)[0]
            with open(self.fem_env_data_dir + 'vel.txt', 'w') as f:
                f.write(str(self.target_vel))
            #np.savetxt(fem_env_data_dir, self.target_vel)
        self.vel = self.target_vel
        self.inverse_problem = inverse_problem

    def step(self, action):
        new_state = self.state
        new_state[action] = 1.
        self.vel =  self.solver([new_state], self.id, ncpu_mp=1, ncpu_solver=self.ncpu)[0]
        #print(self.vel)
        if self.inverse_problem:
            reward = self.reward_weight * (self.vel - self.target_vel)
        else:
            reward = self.reward_weight * (self.target_vel - self.vel)
        #self.reward_map[action] = -1 * self.reward_weight
        self.state = new_state
        if reward > 0:
            is_done = True
            self.target_vel = self.vel
            with open(self.fem_env_data_dir + 'vel.txt', 'w') as f:
                f.write(str(self.target_vel))
            np.savetxt(self.fem_env_data_dir + 'init_state.txt', self.state, fmt='%.0f')
        elif self.state.sum() == self.state_size:
            is_done = True
        else:
            is_done = False
        return new_state, reward, is_done, None

    def reset(self):
        self.state_prior_reset = self.state
        self.state = np.zeros_like(self.state)
        self.action_space.reset()
        #self.reward_map = np.copy(self.reward_map_init)
        return self.state
    
    def get_state(self):
        return self.state.reshape((self.h, self.w))    
